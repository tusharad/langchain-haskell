"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[369],{6045:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>T,contentTitle:()=>S,default:()=>E,frontMatter:()=>N,metadata:()=>t,toc:()=>O});const t=JSON.parse('{"id":"getting-started/quick_start","title":"Quickstart: Langchain-hs","description":"Requirements","source":"@site/docs/getting-started/quick_start.md","sourceDirName":"getting-started","slug":"/getting-started/quick_start","permalink":"/langchain-hs/docs/getting-started/quick_start","draft":false,"unlisted":false,"editUrl":"https://github.com/tusharad/langchain-hs/docs/getting-started/quick_start.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Getting Started","permalink":"/langchain-hs/docs/category/getting-started"},"next":{"title":"Concepts and examples","permalink":"/langchain-hs/docs/category/concepts-and-examples"}}');var r=n(4848),s=n(8453),l=n(6540),i=n(4164),o=n(5627),u=n(6347),c=n(372),p=n(604),d=n(1861),h=n(8749);function g(e){return l.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:a,children:n}=e;return(0,l.useMemo)((()=>{const e=a??function(e){return g(e).map((e=>{let{props:{value:a,label:n,attributes:t,default:r}}=e;return{value:a,label:n,attributes:t,default:r}}))}(n);return function(e){const a=(0,d.XI)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,n])}function f(e){let{value:a,tabValues:n}=e;return n.some((e=>e.value===a))}function L(e){let{queryString:a=!1,groupId:n}=e;const t=(0,u.W6)(),r=function(e){let{queryString:a=!1,groupId:n}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:a,groupId:n});return[(0,p.aZ)(r),(0,l.useCallback)((e=>{if(!r)return;const a=new URLSearchParams(t.location.search);a.set(r,e),t.replace({...t.location,search:a.toString()})}),[r,t])]}function x(e){const{defaultValue:a,queryString:n=!1,groupId:t}=e,r=m(e),[s,i]=(0,l.useState)((()=>function(e){let{defaultValue:a,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!f({value:a,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:a,tabValues:r}))),[o,u]=L({queryString:n,groupId:t}),[p,d]=function(e){let{groupId:a}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(a),[t,r]=(0,h.Dv)(n);return[t,(0,l.useCallback)((e=>{n&&r.set(e)}),[n,r])]}({groupId:t}),g=(()=>{const e=o??p;return f({value:e,tabValues:r})?e:null})();(0,c.A)((()=>{g&&i(g)}),[g]);return{selectedValue:s,selectValue:(0,l.useCallback)((e=>{if(!f({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),d(e)}),[u,d,r]),tabValues:r}}var b=n(9136);const v={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function y(e){let{className:a,block:n,selectedValue:t,selectValue:s,tabValues:l}=e;const u=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),p=e=>{const a=e.currentTarget,n=u.indexOf(a),r=l[n].value;r!==t&&(c(a),s(r))},d=e=>{let a=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const n=u.indexOf(e.currentTarget)+1;a=u[n]??u[0];break}case"ArrowLeft":{const n=u.indexOf(e.currentTarget)-1;a=u[n]??u[u.length-1];break}}a?.focus()};return(0,r.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},a),children:l.map((e=>{let{value:a,label:n,attributes:s}=e;return(0,r.jsx)("li",{role:"tab",tabIndex:t===a?0:-1,"aria-selected":t===a,ref:e=>{u.push(e)},onKeyDown:d,onClick:p,...s,className:(0,i.A)("tabs__item",v.tabItem,s?.className,{"tabs__item--active":t===a}),children:n??a},a)}))})}function A(e){let{lazy:a,children:n,selectedValue:t}=e;const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(a){const e=s.find((e=>e.props.value===t));return e?(0,l.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,r.jsx)("div",{className:"margin-top--md",children:s.map(((e,a)=>(0,l.cloneElement)(e,{key:a,hidden:e.props.value!==t})))})}function k(e){const a=x(e);return(0,r.jsxs)("div",{className:(0,i.A)("tabs-container",v.tabList),children:[(0,r.jsx)(y,{...a,...e}),(0,r.jsx)(A,{...a,...e})]})}function M(e){const a=(0,b.A)();return(0,r.jsx)(k,{...e,children:g(e.children)},String(a))}const j={tabItem:"tabItem_Ymn6"};function I(e){let{children:a,hidden:n,className:t}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,i.A)(j.tabItem,t),hidden:n,children:a})}const N={sidebar_position:1},S="Quickstart: Langchain-hs",T={},O=[{value:"Requirements",id:"requirements",level:3},{value:"Add langchain-hs package in your dependancies",id:"add-langchain-hs-package-in-your-dependancies",level:3},{value:"Example of generating response from a single prompt.",id:"example-of-generating-response-from-a-single-prompt",level:3},{value:"Example of generating response from a chat history",id:"example-of-generating-response-from-a-chat-history",level:3},{value:"Example of streaming response",id:"example-of-streaming-response",level:3}];function w(e){const a={a:"a",admonition:"admonition",code:"code",h1:"h1",h3:"h3",header:"header",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"quickstart-langchain-hs",children:"Quickstart: Langchain-hs"})}),"\n",(0,r.jsx)(a.h3,{id:"requirements",children:"Requirements"}),"\n",(0,r.jsxs)(a.p,{children:["For using ",(0,r.jsx)(a.code,{children:"Ollama LLM"}),", Make sure you have ",(0,r.jsx)(a.a,{href:"https://ollama.com/",children:"Ollama"})," installed."]}),"\n",(0,r.jsx)(a.h3,{id:"add-langchain-hs-package-in-your-dependancies",children:"Add langchain-hs package in your dependancies"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-yaml",metastring:'title="stack.yaml"',children:"dependancies:\n  base,\n  langchain-hs\n"})}),"\n",(0,r.jsx)(a.h3,{id:"example-of-generating-response-from-a-single-prompt",children:"Example of generating response from a single prompt."}),"\n",(0,r.jsxs)(M,{defaultValue:"ollama",values:[{label:"Ollama",value:"ollama"},{label:"OpenAI",value:"openai"},{label:"Huggingface",value:"huggingface"}],children:[(0,r.jsx)(I,{value:"ollama",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule LangchainLib (runApp) where\n\nimport Langchain.LLM.Ollama (Ollama(..))\nimport Langchain.LLM.Core\nimport qualified Data.Text as T\n\nrunApp :: IO ()\nrunApp = do\nlet ollamaLLM = Ollama "llama3.2" []  \ngenResult <- generate ollamaLLM "Explain Haskell in simple terms." Nothing\ncase genResult of\n    Left err -> putStrLn $ "Generate error: " ++ err\n    Right text -> putStrLn $ "Generated Text:\\n" ++ T.unpack text\n'})})}),(0,r.jsx)(I,{value:"openai",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule Main where\n\nimport Data.Text (Text)\nimport qualified Langchain.LLM.Core as LLM\nimport Langchain.LLM.OpenAI (OpenAI(..))\n\nmain :: IO ()\nmain = do\nlet openAI = OpenAI\n      { apiKey = "your-api-key"\n      , openAIModelName = "gpt-4.1-nano"\n      , callbacks = []\n      }\nresult <- LLM.generate openAI "Tell me a joke" Nothing\ncase result of\n  Left err -> putStrLn $ "Error: " ++ err\n  Right response -> putStrLn response\n'})})}),(0,r.jsx)(I,{value:"huggingface",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule LangchainLib (runApp) where\n\nimport qualified Data.Text as T\nimport Langchain.LLM.Core\nimport Langchain.LLM.Huggingface \n\nrunApp :: IO ()\nrunApp = do\n    let huggingface =\n            Huggingface\n                { provider = Cerebras\n                , apiKey = <your-api-key>\n                , modelName = "llama-3.3-70b"\n                , callbacks = []\n                }\n    eRes <- generate huggingface "Explain me Monads in Haskell" Nothing\n    case eRes of\n      Left err -> putStrLn $ "Chat error: " ++ err\n      Right response -> putStrLn $ "Chat Response:\\n" ++ T.unpack response\n'})})})]}),"\n",(0,r.jsxs)(a.p,{children:["In above code, I have intialized the ",(0,r.jsx)(a.code,{children:"Ollama"})," with model name ",(0,r.jsx)(a.code,{children:"llama3.2"}),". Then I can simply call generate function with ollamaLLM and the prompt.\nIt will either return a Error or the response Text."]}),"\n",(0,r.jsx)(a.admonition,{type:"warning",children:(0,r.jsx)(a.p,{children:"For Ollama, Make sure the model that you want to use is installed on your local machine. Else it will throw error."})}),"\n",(0,r.jsx)(a.h3,{id:"example-of-generating-response-from-a-chat-history",children:"Example of generating response from a chat history"}),"\n",(0,r.jsxs)(M,{defaultValue:"ollama",values:[{label:"Ollama",value:"ollama"},{label:"OpenAI",value:"openai"},{label:"Huggingface",value:"huggingface"}],children:[(0,r.jsx)(I,{value:"ollama",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule LangchainLib (runApp) where\n\nimport Langchain.LLM.Ollama (Ollama(..))\nimport Langchain.LLM.Core\nimport qualified Data.Text as T\nimport Data.List.NonEmpty (fromList)\n\nrunApp :: IO ()\nrunApp = do\n  let ollamaLLM = Ollama "llama3.2" []  \n  let chatHistory = fromList\n        [ Message System "Explain everthing with a texas accent." defaultMessageData\n        , Message User "What is functional programming?" defaultMessageData\n        ]\n  chatResult <- chat ollamaLLM chatHistory Nothing\n  case chatResult of\n    Left err -> putStrLn $ "Chat error: " ++ err\n    Right response -> putStrLn $ "Chat Response:\\n" ++ T.unpack response\n'})})}),(0,r.jsx)(I,{value:"openai",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule Main where\n\nimport qualified Langchain.LLM.Core as LLM\nimport qualified Data.Text as T\nimport Data.List.NonEmpty (fromList)\n\nmain :: IO ()\nmain = do\nlet openAI = OpenAI\n      { apiKey = "your-api-key"\n      , openAIModelName = "gpt-4.1-nano"\n      , callbacks = []\n      }\nlet chatHistory = fromList\n      [ Message System "You are an AI assistant." defaultMessageData\n      , Message User "What is functional programming?" defaultMessageData\n      ]\nchatResult <- LLM.chat openAI chatHistory Nothing\ncase chatResult of\n  Left err -> putStrLn $ "Chat error: " ++ err\n  Right response -> putStrLn $ "Chat Response:\\n" ++ T.unpack response\n'})})}),(0,r.jsx)(I,{value:"huggingface",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule LangchainLib (runApp) where\n\nimport qualified Data.Text as T\nimport Langchain.LLM.Core \nimport Langchain.LLM.Huggingface\nimport Data.List.NonEmpty (fromList)\n\nrunApp :: IO ()\nrunApp = do\n    let huggingface =\n            Huggingface\n                { provider = Cerebras\n                , apiKey = <your-api-key>\n                , modelName = "llama-3.3-70b"\n                , callbacks = []\n                }\n    let chatHistory = fromList [ \n            Message System "You are an AI assistant." defaultMessageData\n          , Message User "What is functional programming?" defaultMessageData]\n    eRes <- chat huggingface chatHistory Nothing\n    case eRes of\n      Left err -> putStrLn $ "Chat error: " ++ err\n      Right response -> putStrLn $ "Chat Response:\\n" ++ T.unpack response\n'})})})]}),"\n",(0,r.jsxs)(a.p,{children:["There are several helper functions provided to manipulate ",(0,r.jsx)(a.code,{children:"chatHistory"})]}),"\n",(0,r.jsx)(a.h3,{id:"example-of-streaming-response",children:"Example of streaming response"}),"\n",(0,r.jsxs)(M,{defaultValue:"ollama",values:[{label:"Ollama",value:"ollama"},{label:"OpenAI",value:"openai"},{label:"Huggingface",value:"huggingface"}],children:[(0,r.jsx)(I,{value:"ollama",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule Main where\n\nimport Langchain.LLM.Ollama (Ollama(..))\nimport Langchain.LLM.Core\nimport qualified Data.Text as T\nimport qualified Data.Text.IO as T\nimport Data.List.NonEmpty (fromList)\n\nmain :: IO ()\nmain = do\n  let ollamaLLM = Ollama "llama3.2" []  \n  let chatHistory = fromList\n        [ Message System "You are an AI assistant." defaultMessageData\n        , Message User "What is functional programming?" defaultMessageData\n        ]\n  let handler = StreamHandler T.putStr (putStrLn "Response complete")\n  eRes <- stream ollamaLLM chatHistory handler Nothing \n  case eRes of\n    Left err -> putStrLn $ "Chat error: " ++ err\n    Right _ -> pure ()\n'})})}),(0,r.jsx)(I,{value:"openai",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule LangchainLib (runApp) where\n\nimport Data.List.NonEmpty (fromList)\nimport qualified Data.Text.IO as T\nimport Langchain.LLM.Core as LLM\nimport Langchain.LLM.OpenAI\n\nrunApp :: IO ()\nrunApp = do\n    let openAI =\n            OpenAI\n                { apiKey = "your-api-key"\n                , openAIModelName = "gpt-4.1-nano"\n                , callbacks = []\n                }\n    let chatHistory =\n            fromList\n                [ Message System "You are an AI assistant." defaultMessageData\n                , Message User "What is functional programming?" defaultMessageData\n                ]\n    let streamHandler = StreamHandler {\n        onToken = T.putStr,\n        onComplete = pure ()\n    }\n    chatResult <- LLM.stream openAI chatHistory streamHandler Nothing\n    case chatResult of\n        Left err -> putStrLn $ "Chat error: " ++ err\n        Right _ -> pure ()\n'})})}),(0,r.jsx)(I,{value:"huggingface",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule LangchainLib (runApp) where\n\nimport Data.List.NonEmpty (fromList)\nimport qualified Data.Text.IO as T\nimport Langchain.LLM.Core as LLM\nimport Langchain.LLM.Huggingface\n\nrunApp :: IO ()\nrunApp = do\n    let huggingface =\n            Huggingface\n                { provider = Cerebras\n                , apiKey = "your-api-key"\n                , modelName = "llama-3.3-70b"\n                , callbacks = []\n                }\n    let chatHistory = fromList [Message System "You are an AI assistant." defaultMessageData, Message User "What is functional programming?" defaultMessageData]\n    let streamHandler =\n            StreamHandler\n                { onToken = T.putStr\n                , onComplete = pure ()\n                }\n    eRes <- stream huggingface chatHistory streamHandler Nothing\n    case eRes of\n        Left err -> putStrLn $ "Chat error: " ++ err\n        Right _ -> pure ()\n'})})})]}),"\n",(0,r.jsx)(a.p,{children:"For streaming you are supposed to send a callback function. The callback function will be called for each token generated."})]})}function E(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(w,{...e})}):w(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>l,x:()=>i});var t=n(6540);const r={},s=t.createContext(r);function l(e){const a=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(s.Provider,{value:a},e.children)}}}]);