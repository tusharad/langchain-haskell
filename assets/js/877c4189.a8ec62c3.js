"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[466],{321:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"concepts/LLM","title":"LLM","description":"The LLM typeclass is the cornerstone of langchain-hs. It provides a unified interface for:","source":"@site/docs/concepts/LLM.md","sourceDirName":"concepts","slug":"/concepts/LLM","permalink":"/langchain-hs/docs/concepts/LLM","draft":false,"unlisted":false,"editUrl":"https://github.com/tusharad/langchain-hs/docs/concepts/LLM.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Concepts and examples","permalink":"/langchain-hs/docs/category/concepts-and-examples"},"next":{"title":"Document Loaders","permalink":"/langchain-hs/docs/concepts/DocumentLoader"}}');var r=t(4848),a=t(8453);const i={sidebar_position:1},o="LLM",c={},l=[{value:"Supported Integrations",id:"supported-integrations",level:2},{value:"Custom",id:"custom",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"llm",children:"LLM"})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"LLM"})," typeclass is the cornerstone of langchain-hs. It provides a unified interface for:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Text generation"}),"\n",(0,r.jsx)(n.li,{children:"Chat conversations"}),"\n",(0,r.jsx)(n.li,{children:"Streaming responses"}),"\n",(0,r.jsx)(n.li,{children:"Custom parameter handling"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Every type that has LLM instance, can call generate, chat and stream."}),"\n",(0,r.jsx)(n.h2,{id:"supported-integrations",children:"Supported Integrations"}),"\n",(0,r.jsx)(n.p,{children:"At this moment, following integrations available,"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Ollama"}),"\n",(0,r.jsx)(n.li,{children:"OpenAI"}),"\n",(0,r.jsx)(n.li,{children:"HuggingFace"}),"\n",(0,r.jsx)(n.li,{children:"More to come..."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"custom",children:"Custom"}),"\n",(0,r.jsx)(n.p,{children:"It is also possible to create your own type and implement LLM class."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-haskell",children:"class LLM a where\n  generate :: a -> Text -> Maybe Params -> IO (Either String Text)\n  chat :: a -> NonEmpty Message -> Maybe Params -> IO (Either String Text)\n  stream :: a -> NonEmpty Message -> StreamHandler -> Maybe Params -> IO (Either String ())\n"})}),"\n",(0,r.jsx)(n.p,{children:"for e.g"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-haskell",children:'\ndata Deepseek = Deepseek {\n  apiKey :: Text,\n  apiUrl :: Text,\n  model :: Text\n}\n\ninstance LLM Deepseek where\n  generate (Deepseek apiKey apiUrl model) prompt params = do\n    -- Your implementation here\n    return $ Right "Generated text"\n  chat (Deepseek apiKey apiUrl model) messages params = do\n    -- Your implementation here\n    return $ Right "Chat response"\n  stream (Deepseek apiKey apiUrl model) messages handler params = do\n    -- Your implementation here\n    return $ Right ()\n\n'})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var s=t(6540);const r={},a=s.createContext(r);function i(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);