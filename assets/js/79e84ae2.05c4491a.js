"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[260],{8453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>a});var t=o(6540);const s={},r=t.createContext(s);function i(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(r.Provider,{value:n},e.children)}},9716:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>l,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"concepts/Memory","title":"Memory","description":"In langchain-hs, Memory module provides a BaseMemory typeclass. The goal is provide types with","source":"@site/docs/concepts/Memory.md","sourceDirName":"concepts","slug":"/concepts/Memory","permalink":"/langchain-hs/docs/concepts/Memory","draft":false,"unlisted":false,"editUrl":"https://github.com/tusharad/langchain-hs/docs/concepts/Memory.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Embeddings models","permalink":"/langchain-hs/docs/concepts/Embeddings"},"next":{"title":"Output Parser","permalink":"/langchain-hs/docs/concepts/OutputParser"}}');var s=o(4848),r=o(8453);const i={sidebar_position:4},a="Memory",c={},d=[{value:"WindowBufferMemory",id:"windowbuffermemory",level:2},{value:"TokenBufferMemory",id:"tokenbuffermemory",level:2},{value:"Example",id:"example",level:3}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"memory",children:"Memory"})}),"\n",(0,s.jsxs)(n.p,{children:["In langchain-hs, Memory module provides a ",(0,s.jsx)(n.code,{children:"BaseMemory"})," typeclass. The goal is provide types with\nBaseMemory instance, that can easily access and manipulate ",(0,s.jsx)(n.code,{children:"Chat History"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"So far, below BaseMemory instances are defined:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"WindowBufferMemory"}),"\n",(0,s.jsx)(n.li,{children:"TokenBufferMemory"}),"\n",(0,s.jsx)(n.li,{children:"More to come..."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"windowbuffermemory",children:"WindowBufferMemory"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"WindowBufferMemory"})," type provides a mechanism to store chat history with a ",(0,s.jsx)(n.code,{children:"max window size"}),". Once it the conversation crosses that limit, the Type will omit the older conversation and keep the window size intact.\nIt is basic but helpful mechanism to keep the token size."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://python.langchain.com/v0.1/docs/modules/memory/types/buffer_window/",children:"Here"})," is the python documentation which WindowBufferMemory is inspired from."]}),"\n",(0,s.jsx)(n.h2,{id:"tokenbuffermemory",children:"TokenBufferMemory"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"TokenBufferMemory"})," type provides a mechanism to store chat history with a ",(0,s.jsx)(n.code,{children:"max token size"}),". Once it the conversation crosses that limit, the Type will omit the older conversation and keep the token size intact. 1 token is equal to 4 characters as per (link)[",(0,s.jsx)(n.a,{href:"https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them",children:"https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"}),"]."]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"WindowBufferMemory and TokenBufferMemory trims non System messages only."})}),"\n",(0,s.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-haskell",children:'{-# LANGUAGE OverloadedStrings #-}\n\nmodule LangchainLib (runApp) where\n\nimport Langchain.LLM.Core\nimport Langchain.LLM.Ollama\nimport qualified Data.List.NonEmpty as NE\nimport Langchain.Memory.TokenBufferMemory\nimport Langchain.Memory.Core\n\nrunApp :: IO ()\nrunApp = do\n    let o = Ollama "qwen3:4b" []\n    let tokenBuffMsgs = TokenBufferMemory {\n        maxTokens = 30\n      , tokenBufferMessages = NE.fromList [\n          Message User "Hey! how are you? /no_think" defaultMessageData\n      ]\n    }\n    eMsgs <- messages tokenBuffMsgs\n    case eMsgs of\n      Left _ -> pure () \n      Right msgs -> do\n        e <-\n            chat\n                o\n                msgs\n                Nothing\n        print e\n'})})]})}function l(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);